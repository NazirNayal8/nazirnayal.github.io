<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Bayesian Networks | Nazir Nayal </title> <meta name="author" content="Nazir Nayal"> <meta name="description" content="A simple explanation of the basics of Bayesian Networks"> <meta name="keywords" content="nazir, nayal, KU, mpi, mpi-inf, mpi-sws, AI, deep learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="nazirnayal.github.io/blog/2020/bayesian-networks/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Nazir</span> Nayal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Bayesian Networks</h1> <p class="post-meta"> Created in December 11, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> ai</a>   <a href="/blog/tag/tutorial"> <i class="fa-solid fa-hashtag fa-sm"></i> tutorial</a>   ·   <a href="/blog/category/tutorial"> <i class="fa-solid fa-tag fa-sm"></i> tutorial</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="bayesian-networks">Bayesian Networks</h1> <p>A method used to represent Joint Distributions implicitly using conditional independence and local interactions.</p> <h1 id="representation">Representation</h1> <p>It is represented as a directed acyclic graph. Each variable is a Node, and a directed edge from node <code class="language-plaintext highlighter-rouge">A</code> to node <code class="language-plaintext highlighter-rouge">B</code> means that the random variable <code class="language-plaintext highlighter-rouge">B</code> is a function of random variable <code class="language-plaintext highlighter-rouge">A</code>. In other words, a random variable is a function of the random variables in its parent nodes.</p> <p>Every node <code class="language-plaintext highlighter-rouge">A</code> stores a table that represents the conditional probability of random variable <code class="language-plaintext highlighter-rouge">A</code> given every combination of values its parents could take.</p> \[P(A | Parents(A))\] <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/bayes_net_example.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/bayes_net_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h1 id="from-conditional-to-joint">From Conditional to Joint</h1> <p>In order to extract the joint distribution from the local conditionals, we can use the following formula:</p> \[P(X_1, X_2, \dots , X_n) = \prod_{i=1}^n P(X_i|Parents(X_i))\] <h1 id="size-of-a-bayesian-network">Size of a Bayesian Network</h1> <p>In order to store a table that represents an ordinary joint distribution of <code class="language-plaintext highlighter-rouge">N</code> variables where each variable can take <code class="language-plaintext highlighter-rouge">d</code> values, one would need $O(d^N)$ entries which is too large.</p> <p>One important advantage of Bayesian Networks is that they reduce this size needed to store a join distribution by storing different small parts from which we can reconstruct the joint distribution. Every node contains a table that determines its value for every combination of values of its parents. If we assume that every node has no more than <code class="language-plaintext highlighter-rouge">k</code> parents, this would mean that every node has a table of size $O(d^{k+1})$ ($k + 1$ comes from the fact that we encode the parents and the node itself), so for <code class="language-plaintext highlighter-rouge">N</code> nodes, we would get a total of $O(Nd^k)$, which is a reduction that depends on <code class="language-plaintext highlighter-rouge">k</code>, the maximum number of possible parents.</p> <h1 id="does-a-bayesian-network-graph-imply-causality-">Does a Bayesian Network Graph imply Causality ?</h1> <p>No, not all the time. It is easier to think of the graph as if it means that a certain variable (parent) is causing another variable (child), but that is not always the case. Rather, it only means that there is a direct conditional dependency between variables.</p> <h1 id="how-to-check-for-independence-in-a-bayesian-networks">How to check for Independence in a Bayesian Networks</h1> <p>If two variables do not have a direct connection, this does <strong>not</strong> mean that they are independent, they might indirectly influence each other through other nodes.</p> <p>For example in this graph below, <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> are not necessarily independent because they can influence each other through <code class="language-plaintext highlighter-rouge">B</code>.</p> <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/causal_chain.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/causal_chain.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>Nevertheless, we can prove <strong>conditional independence</strong> of some cases. For example, in the network above, we can say that <code class="language-plaintext highlighter-rouge">C</code> is conditionally independent from <code class="language-plaintext highlighter-rouge">A</code> given <code class="language-plaintext highlighter-rouge">B</code>. The intuition is that since a value of <code class="language-plaintext highlighter-rouge">B</code> was given, it already holds the influence from <code class="language-plaintext highlighter-rouge">A</code>, or another way to think of it that the influence between <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">C</code> was blocked by <code class="language-plaintext highlighter-rouge">B</code>.</p> <p>There are 3 types of simple 3-node graphs which are simple to analyze and will be used to detect properties in Bayesian Networks. These graphs are <strong>causal chains</strong>, <strong>common cause</strong>, and <strong>common effect</strong>.</p> <h2 id="causal-chains">Causal Chains</h2> <p>From this structure we can only say that <code class="language-plaintext highlighter-rouge">C</code> is conditionally independent of <code class="language-plaintext highlighter-rouge">A</code> given <code class="language-plaintext highlighter-rouge">B</code>. (can be proven mathematically)</p> <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/causal_chain.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/causal_chain.png" class="img-fluid rounded z-depth-1" width="60%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="common-cause">Common Cause</h2> <p>Here, we can say that <code class="language-plaintext highlighter-rouge">B</code> and <code class="language-plaintext highlighter-rouge">C</code> are independent given <code class="language-plaintext highlighter-rouge">A</code>. (can be proven mathematically)</p> <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/common_cause.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/common_cause.png" class="img-fluid rounded z-depth-1" width="200" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="common-effect">Common Effect</h2> <p>Here, <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> are in fact fully independent, but they are not independent given <code class="language-plaintext highlighter-rouge">C</code>.</p> <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/common_effect.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/common_effect.png" class="img-fluid rounded z-depth-1" width="200" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h1 id="detect-if-2-nodes-are-conditionally-independent-from-the-graph-structure">Detect if 2 nodes are conditionally independent from the graph structure</h1> <p>We can do this by tracing the undirected path between any two nodes on the graph. For every 3 consecutive nodes on the path, we check if they satisfy conditional independence. In order to do the check, we simply identify to which type this triplet belongs (causal chain, common cause, common effect). If we find any triplet on the path to voilate independence then we assume that conditional independence between source and target node is <strong>not guaranteed</strong>. The triplets which imply conditional independence are called <strong>Inactive Triplets</strong>, whereas triplets that violate conditional independence are called <strong>Active Triplets</strong>. The following figure summarizes the triplet cases.</p> <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/triplets.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/triplets.png" class="img-fluid rounded z-depth-1" width="400" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>This approach is called <strong>D-Separation</strong>.</p> <h1 id="markov-blanket">Markov Blanket</h1> <p>For a given node <code class="language-plaintext highlighter-rouge">X</code>, we can say that it is conditionally independent of all other nodes given its Markov Blanket. A <strong>Markove Blanket</strong> for a node consists of its parents, children, and children’s parents. Like in the image below.</p> <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/markov_blanket.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/markov_blanket.png" class="img-fluid rounded z-depth-1" width="500" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h1 id="probabilistic-inference">Probabilistic Inference</h1> <p>The act of calculating the probability that some random variables take certain values from a join distribution.</p> <h1 id="inference-by-enumeration">Inference by Enumeration</h1> <p>This is basically the brute force way of calculating a certain probability, where we first take the sum of the probabilities of all the hidden random variables to eliminate them, and then calculate the desired probability of the target and evidence.</p> <p><strong>Note</strong>: Hidden variables are the rest of the random variables which are neither in the target set, nor in the evidence set. Evidence contains the random variables which make the condition of the probability calculated.</p> <p>Enumeration gives an exact answer but is exponential in complexity.</p> <p>Steps of performing Inference by Enumeration</p> <ol> <li>From each probability table, select the entries whose rows are consistent with the evidence. That is, if we have two variables <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> in the evidence, and both take values <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> respectively, then select all rows for which variables <code class="language-plaintext highlighter-rouge">A</code> and <code class="language-plaintext highlighter-rouge">B</code> have values <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code> respectively.</li> <li>For all the hidden variables, sum out their values. This means compute the sum of probabilities for all combinations of hidden variables values. This operation is called <strong>marginalization</strong>. \(P(Q, e_1 \dots e_n) = \sum_{h_1 \dots h_r} P(Q, h_1 \dots h_r, e_1 \dots e_k)\)</li> <li>Normalize the the the values left so that their sum is 1 and they form a valid distribution.</li> </ol> \[Z = \sum_q P(Q, e_1 \dots e_k) \space \space , \space \space P(Q | e_1 \dots e_k) = \frac{1}{Z} P(Q | e_1 \dots e_k)\] <h1 id="variable-elimination">Variable Elimination</h1> <p>In enumeration, what we did was build the whole join distribution and then marginalize in order to reduce it to the desired state of target and evidence variables. The idea suggested by Variable Elimination to speed up the calculations is to interleave between joining and marginalizing. It basically remains exponential but it helps speed up the calculations compared to enumerations.</p> <p>In order to understand how Variable Elimination works, we need to understand the concept of <strong>Factors</strong> first.</p> <h2 id="factors">Factors</h2> <p>Factorization as a mathematical concept means to decompose an object into a product of other objects called factors.</p> <p><strong>Note</strong>: Capital letters in a distribution determine its dimensions, because small letters denote assigned values.</p> <p>For Bayesian Networks, let’s see what kind of factors there can be:</p> <ul> <li>Joint Distribution $P(X,Y)$ <ul> <li>Contains entries $P(x,y)$ for all $x,y$</li> <li>Sums to 1</li> </ul> </li> <li>Selected Joint $P(x, Y)$ <ul> <li>A slice of the joint Distribution</li> <li>Sums to $P(x)$, because it represent probability of $x$ for every value of $y$, which means that $P(x)$ is sort of partitioned over $y$ values.</li> </ul> </li> <li>Single Conditional $P(Y|x)$ <ul> <li>Contains entries $P(y, x)$ for all $y$ and fixed $x$</li> <li>Sums to 1, because it is basically a conditional probability distribution for all values of y conditioned on a single value $x$.</li> </ul> </li> <li>Family of Conditionals $P(Y|X)$ <ul> <li>Contains multiple conditionals.</li> <li>Contains Entries $P(y|x)$ for all $x,y$</li> <li>sums to $|X|$, which is the number of values that can be taken by the random variable $X$. This is because for every value $X$, we have a conditional distribution for $Y$ over that particular $x$, which means we essentially have $|X|$ distributions where each one sums to 1.</li> <li>Specified Family $P(y|X)$ <ul> <li>Contains entries $P(y|x)$ for fixed $y$, but for all $x$.</li> <li>Sums to an unknown value.</li> </ul> </li> </ul> </li> </ul> <p>So now back to Variable Elimination, we will use 2 main operations, <strong>join</strong> and <strong>eleminate</strong>. Join operation basically joins two tables in a similar way to the database join operations. In simpler terms, if table <code class="language-plaintext highlighter-rouge">A</code> has two variables <code class="language-plaintext highlighter-rouge">X</code> and <code class="language-plaintext highlighter-rouge">Y</code>, and table <code class="language-plaintext highlighter-rouge">B</code> has two variables <code class="language-plaintext highlighter-rouge">Y</code> and <code class="language-plaintext highlighter-rouge">Z</code>, then a join operation would result in a 3 dimensional table of <code class="language-plaintext highlighter-rouge">X</code>, <code class="language-plaintext highlighter-rouge">Y</code>, and <code class="language-plaintext highlighter-rouge">Z</code>. where probability of an entry in table <code class="language-plaintext highlighter-rouge">A</code> is multiplied by the a probability of table <code class="language-plaintext highlighter-rouge">B</code> where the common variable <code class="language-plaintext highlighter-rouge">Y</code> values match. Eliminate operations is simply summing for all the values of a certain variable to remove it from the table.</p> <ol> <li>Select all rows that match the evidence and delete rows that do not match it, that is only if a table contains an evidence variable. If a table does not contain an evidence variable don’t touch it. Here are the steps to follow in order to solve a query using Variable Elimination:</li> <li>Since we cannot eliminate a hidden variable unless it exists in only one facts, we join factors which have a common hidden variable.</li> <li>Then for every factor which contains a hidden variable and that hidden variable only exists in that factor, eliminate the hidden variable.</li> <li>Repeat steps 2 and 3 until the factor left consists of target and evidence.</li> <li>Finally we normalize the table we get, because the existence of evidence variables might cause a selected joint to appear.</li> </ol> <p><strong>Note:</strong> When joining two tables where each table contains some variables on the left side of the condition and variables on right side, the trick is to place all variables on the left side of the two tables to the left side of the output table, and then place the rest on the right. This would save time in a case where a variable exists on the left side of a table and the right side of another table. To understand what I mean by left and right: $P(X|Y)$, here $X$ is the left of the condition and $Y$ is the right of the condition.</p> <p><strong>Conclusion of Variable Elimination</strong>: The worst case complexity of Variable Elimination is not better than Enumeration in theory but in practice it is faster. This difference in speed is due to the fact that eliminating some variables in a certain order decreases the <strong>maximum factor generated</strong>, which affects the computations significantly if we were able to find a good ordering.</p> <h1 id="approximate-inference">Approximate Inference</h1> <p>Since it takes too long to calculate exact inferences using the method discussed earlier, we may sacrifice some of the exactness in order to speed up the calculation. One method to do this is <strong>Sampling</strong>.</p> <h1 id="sampling">Sampling</h1> <p>The idea behind sampling is to draw $N$ samples from a distribution and then use these samples to compute an approximate posterior probability and show that this can converge to the true probability. <strong>How can we computer a posterior probability from samples ?</strong> Simply, we divide the number of samples matching the target over the samples matching the evidence, a very basic way of calculating probability.</p> <h2 id="how-to-sample-from-a-known-distribution">How to sample from a known distribution</h2> <p>Since it is a known distribution, then for each value the random variable takes we know its probability. Hence, we do the following steps:</p> <ol> <li>Get a sample $u$ from the uniform distribution over the interval [0, 1) (assume that we have the mechanism to do so).</li> <li>For every value of the random variable assign an interval in the range [0, 1) which is of length equal to the probability at that value. We can do this because the sum of all probabilities is equal to 1.</li> <li>Choose the value of the random variable that has the sample $u$ within it.</li> <li>Repeat this process $n$ times to get $n$ samples.</li> </ol> <p>Four main sampling methods will be discussed</p> <ul> <li>Prior Sampling</li> <li>Rejection Sampling</li> <li>Likelihood Weighting</li> <li>Gibbs Sampling (Most used)</li> </ul> <h1 id="prior-sampling">Prior sampling</h1> <p>To apply prior sampling on a Bayesian Network:</p> <ol> <li>Sort the nodes in the graph using topological sorting.</li> <li>Iterate over the variables in the sorted order and randomly sample from each. Sampling from a node would affect sampling of its children, because if a random variable <code class="language-plaintext highlighter-rouge">A</code> gets a value <code class="language-plaintext highlighter-rouge">a</code>, then when sampling from its child <code class="language-plaintext highlighter-rouge">B</code>, we sample from the distribution that is conditioned on <code class="language-plaintext highlighter-rouge">A</code> taking the value <code class="language-plaintext highlighter-rouge">a</code>. Topological sorting guarantees that a node will not be sampled until all its parents are sampled, which determines the value combinations the child needs to sample from.</li> <li>When the last node is reached, we would have gained a single sample of our Bayesian Network. So we need to repeat the steps $n$ times to get $n$ samples.</li> <li>Once we get $n$ samples, we eliminate the samples which do not match the evidence, then the desired probability would be the number of samples which match the target divided by the number of samples left.</li> </ol> <p>As $n$ goes to infinity, then the probabilities computer using Prior Sampling would converge to the exact original probabilities. This is why we call this sampling method <strong>consistent</strong>.</p> <h1 id="rejection-sampling">Rejection Sampling</h1> <p>The idea in Rejection Sampling is simple, given a desired query with targets and evidence, for every sample we get, we reject it if it does not match our desired evidence. That is, if we want to collect $n$ samples, we keep drawing samples such that all the $n$ chosen samples have our evidence satisfied, so we might perform more than $n$ sampling operations to get $n$ samples which match our evidence. The number of sampling operations can be very high if our evidence is unlikely because we would reject many of the samples we get.</p> <h1 id="likelihood-weighting">Likelihood Weighting</h1> <p>Rejection Sampling has a very serious limitation, which is the fact that many samples can be rejected if the evidence was unlikely. Likelihood Weighting solves this issue with the following steps:</p> <ol> <li>Fix the evidence variables and sample from the others instead of rejecting.</li> <li>Fixing evidence alone would disrupt the distribution and make it inconsistent. Therefore, we modify the sampling algorithm maintain and return a constant weight which is initially set to 1.</li> <li>As we move through the network in topological order, if we encounter an evidence variable, we multiply its probability give its parents with the weight constant we have and update the value of the weight to the new value (old value times probability of evidence given its parents).</li> <li>The sampling algorithm returns the sample with accumulated weight, so in this way every sample will be associated with a certain weight.</li> </ol> <p>Likelihood Weighting is <strong>consistent</strong> because the product of Sampling distribution that is a result of fixing evidence and the weight of every sample (which is an accumulated product of evidence variables given their parents) gives us the original join distribution (Mathematical equations to be added here later).</p> <p><strong>Note:</strong> After generating the samples, instead of counting the number of samples satisfying the target, we sum the weights instead. This is essential to produce a consistent answer.</p> <p><strong>Issues of Likelihood Weighting</strong>: Fixing the evidence affects the downstream variables (variables which are reachable from an evidence in the directed graph and come after it in the topological sorting), but it does not affect the upstream variables (variables which can reach the evidence in the directed graph and come before it in the topological sorting). this is bad because when fixing the evidence, all its downstream will be affected by its value but it would still follow the distribution. On the other hand, fixing the value of an evidence regardless of its parents values creates an inconsistency which we approximately solve by using the weight approach. Therefore, the larger the number of upstream variables, the less accurate the approximation is. Following this logic, we prefer evidences that exist at the top of the hierarchy rather than the bottom.</p> <h1 id="gibbs-sampling">Gibbs Sampling</h1> <p>Gibbs Sampling addresses the issue with Likelihood Weighting, which is that the upstream variables of a fixed evidence are sampled independently from the fixed evidence. Gibbs Sampling attempts to sample all variables taking evidence into consideration.</p> <p>Approach to Generate a Single Sample:</p> <ol> <li>Start with a random assignment to each variable except for the evidence variables which are already fixed with their pre-determined values.</li> <li>Sample each non-evidence variable one variable at a time (Round Robin Style), conditioned on all the other variables while keeping the evidence fixed.</li> <li>Repeat this for a long time.</li> </ol> <p>This approach has the property that repeating it infinitely many times results in a sample that comes from the correct distribution. This way we also guarantee that both upstream and downstream variables are conditioned on evidence.</p> <p><strong>One might ask</strong>: How efficient is it to sample one variable conditioned on all the rest ? Well, the probability of a variable conditioned on all the rest is equal to the joint probability divided on the probability of all the rest. In this equation many terms would cancel out between the numerator and denominator. More specifically, all Conditional Probability Tables which do not contain the re-sampled variables will be canceled out. In other words, to re-sample a variable we simply join the tables that contain it and use them for the calculations, which can be faster if the local interactions in the Bayesian Network were limited.</p> <p><strong>Note</strong>: Sampling a variable given its Markov Blanket is enough.</p> <p>As with the previous methods, once we have the samples, probabilities are computer by counting samples that match the target and divide by the number of samples (because they all match the evidence by definition).</p> <p><strong>Note</strong>: Gibbs Sampling is special case of a more general method called <strong>Markov Chain Monte Carlo (MCMC)</strong> methods. <strong>Metropolis-Hastings</strong> is another famous MCMC method.</p> <h1 id="decision-networks">Decision Networks</h1> <p>They are an extension of Bayesian Networks where we assign utilities to outcomes to help make decisions. Two new types of nodes are added to the Bayesian Network:</p> <ul> <li>Actions (Rectangles): cannot have parents because agents decide on their actions.</li> <li>Utility Node (Diamond): depends on action and chance nodes.</li> </ul> <div class="col-sm mt-3 mt-md-0" align="center"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bayes_net/decision_net.png" sizes="95vw"></source> <img src="/assets/img/bayes_net/decision_net.png" class="img-fluid rounded z-depth-1" width="340" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>How to select an action in this network:</p> <ol> <li>Instantiate all evidence variables.</li> <li>Calculate Posterior of all parents of the utility node.</li> <li>For every possible action, calculate the expected utility.</li> <li>Choose Action with maximum utility. This is called the <strong>Maximum Expected Utility (MEU)</strong>.</li> </ol> <h1 id="value-of-information">Value of Information</h1> <p>Finding the value of discovering new information or in the case of Bayesian Network adding new evidence values can help in deciding to which direction we must put effort in order to collect data. Therefore we attempt to calculate the Value of Information to guide our choices and decisions better.</p> <p><strong>How to calculate the Value of Information ?</strong></p> <ol> <li>Calculate the Maximum Expected Utility (MEU) before acquiring the new evidence.</li> <li>Calculate expected MEU after acquiring the new evidence</li> <li>Value of Information is equal to new expected MEU minus old MEU.</li> <li>If Value of Information is positive, then it would by worthwhile adding this evidence.</li> </ol> <p><strong>How to calculate the Expected MEU after acquiring new evidence</strong> ?</p> <ol> <li>Assume evidence is added</li> <li>For every value of the evidence, calculated expected utility conditioned on that value.</li> <li>For every value of the evidence, multiply probability of evidence with expected utility conditioned on that evidence and sum all these values. The result is the expected MEU.</li> </ol> <p>Properties of Value of Information (VPI):</p> <ul> <li>Non-negative: we can never get a negative VPI.</li> <li>Non-Additive</li> </ul> \[VPI(E_i, E_j \| e) \neq VPI(E_i \| e) + VPI(E_j \| e)\] <ul> <li>Order Independent: If you have two evidences to check their values, it doesn’t matter in which order they are calculated.</li> </ul> \[VPI(E_i, E_j \| e) = VPI(E_i \| e) + VPI(E_j \| e, E_i) = VPI(E_j \| e) + VPI(E_i \| e, E_j)\] <p><strong>Note</strong>: $VPI$ stands for Value of Perfect Information.</p> <p>We can have some shortcuts in calculating VPO through some independence properties:</p> <ul> <li>If $Parents(U)$ ($U$ is utility node) is independent from a node <code class="language-plaintext highlighter-rouge">Z</code> given some evidence, then $VPI(Z | evidence) = 0$</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/a-dive-into-3d-reconstruction-literature/">A Dive into 3D Reconstruction Literature</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/road-to-3dgs/">Road to 3D Gaussian Splatting</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"NazirNayal8/nazirnayal.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Nazir Nayal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"All of my publications in chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"showcasing my projects that I have enjoyed working on",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-cv",title:"CV",description:"You may download my CV from the link with pdf icon on the right",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-repositories",title:"repositories",description:"Favorite Github Repos.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"post-a-dive-into-3d-reconstruction-literature",title:"A Dive into 3D Reconstruction Literature",description:"A summary and literature review of the solutions proposed to solve the 3D reconstruction problem",section:"Posts",handler:()=>{window.location.href="/blog/2024/a-dive-into-3d-reconstruction-literature/"}},{id:"post-road-to-3d-gaussian-splatting",title:"Road to 3D Gaussian Splatting",description:"A simplified and self-contained tutorial for understanding 3D Gaussian Splatting [Under Construction]",section:"Posts",handler:()=>{window.location.href="/blog/2024/road-to-3dgs/"}},{id:"post-bayesian-networks",title:"Bayesian Networks",description:"A simple explanation of the basics of Bayesian Networks",section:"Posts",handler:()=>{window.location.href="/blog/2020/bayesian-networks/"}},{id:"news-joined-the-ai-team-of-hospital-on-mobile-https-www-carex-ai-now-carex-ai-as-an-intern",title:"Joined the AI team of [Hospital on Mobile](https://www.carex.ai/) (now CareX.ai) as an intern...",description:"",section:"News"},{id:"news-joined-kuis-ai-center-as-a-msc-student-working-with-fatma-g\xfcney-https-mysite-ku-edu-tr-fguney-and-jo\xe3o-f-henriques-https-www-robots-ox-ac-uk-joao",title:"Joined KUIS AI Center as a MSc Student working with [Fatma G\xfcney](https://mysite.ku.edu.tr/fguney/) and...",description:"",section:"News"},{id:"news-our-paper-quot-rba-segmenting-unknown-regions-rejected-by-all-https-kuis-ai-github-io-rba-quot-was-accepted-at-iccv-2023",title:"Our paper &quot; [RbA: Segmenting Unknown Regions Rejected by All](https://kuis-ai.github.io/RbA/)&quot; was accepted at...",description:"",section:"News"},{id:"news-defended-by-msc-thesis-titled-quot-rba-segmenting-unknown-regions-rejected-by-all-using-mask-classifiers-quot",title:"Defended by MSc Thesis titled &quot;RbA: Segmenting Unknown Regions Rejected by All using...",description:"",section:"News"},{id:"news-started-as-a-graduate-student-visitor-at-vgg-oxford-https-www-robots-ox-ac-uk-vgg",title:"Started as a Graduate Student Visitor at [VGG Oxford](https://www.robots.ox.ac.uk/~vgg/).",description:"",section:"News"},{id:"news-our-new-paper-quot-a-likelihood-ratio-based-approach-to-segmenting-unknown-objects-https-arxiv-org-abs-2409-06424-quot-is-out",title:"Our new paper &quot;[A Likelihood Ratio-Based Approach to Segmenting Unknown Objects](https://arxiv.org/abs/2409.06424)&quot; is out!...",description:"",section:"News"},{id:"news-joined-the-computer-vision-and-machine-learning-department-at-the-max-planck-institute-for-informatics-mpi-inf-https-www-mpi-inf-mpg-de-departments-computer-vision-and-machine-learning-as-a-phd-student-through-the-cs-max-planck-https-www-cis-mpg-de-doctoral-program",title:"Joined the Computer Vision and Machine Learning department at the [Max Planck Institute...",description:"",section:"News"},{id:"news-joined-the-geometric-representation-learning-group-https-geometric-rl-mpi-inf-mpg-de-people-html-for-a-research-immersion-lab-under-the-supervision-of-jan-eric-lenssen-https-geometric-rl-mpi-inf-mpg-de-people-lenssen-html",title:"Joined the [Geometric Representation Learning Group](https://geometric-rl.mpi-inf.mpg.de/people.html) for a Research Immersion Lab under the...",description:"",section:"News"},{id:"projects-air-traffic-control-simulation",title:"Air Traffic Control Simulation",description:"Simulating air traffic control in an airport modeled as a resource allocation problem.",section:"Projects",handler:()=>{window.location.href="/projects/01_air_traffic_control/"}},{id:"projects-amazon-fine-food-reviews-sentiment-analysis",title:"Amazon Fine Food Reviews Sentiment Analysis",description:"Multi-Label Text Classification using LSTM Neural Network",section:"Projects",handler:()=>{window.location.href="/projects/02_amazon_review_sentiment/"}},{id:"projects-bricking-bad-brick-breaking-game",title:"Bricking Bad Brick Breaking Game",description:"An exciting desktop game with bricks, aliens, power-ups, and more interesting stuff.",section:"Projects",handler:()=>{window.location.href="/projects/03_bricking_bad/"}},{id:"projects-file-system-memory-allocation-algorithms",title:"File System Memory Allocation Algorithms",description:"Implementation of Contiguous Allocation and Linked Allocation algorithms for file system memory management.",section:"Projects",handler:()=>{window.location.href="/projects/04_memory_allocation/"}},{id:"projects-lightchain-proof-of-concept",title:"LightChain Proof of Concept",description:"An implementation of LightChain System, which is DHT-based permission-less blockchain.",section:"Projects",handler:()=>{window.location.href="/projects/05_lightchain/"}},{id:"projects-skip-graph-middlware-implementation",title:"Skip Graph Middlware Implementation",description:"Open-Source Implementation of Skip Graph Middelware.",section:"Projects",handler:()=>{window.location.href="/projects/06_skipgraph_middleware/"}},{id:"projects-rba-segmenting-unknown-regions-rejected-by-all",title:"RbA: Segmenting Unknown Regions Rejected By All",description:"A Deep Learning-based method for segmenting unknown objects on the road.",section:"Projects",handler:()=>{window.location.href="/projects/07_RbA/"}},{id:"projects-likelihood-ratio-for-unknown-object-segmentation",title:"Likelihood-Ratio for Unknown Object Segmentation",description:"Open-Source Implementation of Skip Graph Middelware.",section:"Projects",handler:()=>{window.location.href="/projects/08_UEM/"}},{id:"projects-moodymuch",title:"MoodyMuch",description:"A cross-platform media recommendation app based on mood",section:"Projects",handler:()=>{window.location.href="/projects/09_moodymuch/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/zzz2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/zzz3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/zzz4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/zzz5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/zzz6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/zzz7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/zzz8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/zzz9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6E%6E%61%79%61%6C@%6D%70%69-%69%6E%66.%6D%70%67.%64%65","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=CVYjW50AAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/1833512578","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/NazirNayal8","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/nazirnayal","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/NazirNayal","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>