---
layout: distill
title: Road to 3D Gaussian Splatting
description: A simplified and self-contained tutorial for understanding 3D Gaussian Splatting [Under Construction]
tags: ai tutorial 3d computer-vision
giscus_comments: true
date: 2024-10-04
featured: False
related_publications: true

authors:
  - name: Nazir Nayal
    url: "nazirnayal.xyz"
    affiliations:
      name: MPI-INF

bibliography: 2024-10-04-road-to-3dgs.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - sidebar: left
 
# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
# _styles: >
#   .fake-img {
#     background: #bbb;
#     border: 1px solid rgba(0, 0, 0, 0.1);
#     box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
#     margin-bottom: 12px;
#   }
#   .fake-img p {
#     font-family: monospace;
#     color: white;
#     text-align: left;
#     margin: 12px 0;
#     text-align: center;
#     font-size: 16px;
#   }
---

# Introduction

  The end goal of this blog is providing the necessary c and explanations to understand 3D Gaussian Splatting. I created this blog for my own learning and to simplify learning for others as well without the need to go around the internet gathering and intersecting the pieces of knowledge. This tutorial is based on several readings from different sources which will be cited with links.

# Problem Overview

  3D Gaussian Splatting is a deep learning method to create an implicit 3D representation of a scene, which allows for projecting the scene on to a 2D surface from different view points.The 3D representation is learned from a view available images of the same scene from different view points, along with their camera position information. The goal is to make the representation general enough so that we can project the scene on to novel view points.

---

# Neural Radiance Fields (NeRFs)


  In order to train a NeRF, we need to construct a dataset of $$N$$ images, each with its corresponding camera position information. Then, a neural network, (usually an MLP) is trained to take 5 coordinates $$(x, y, z, \theta, \phi)$$ as input, where $$(x, y, z)$$ are the 3D location information, and 
<!-- TODO: Validate the definition of theta and phi -->
  $$(\theta, \phi)$$ are the angles determining the view point angle. 
  The network outputs the color and density values for each pixel of an image of a particular view. The network hence can be optimized by matching the ground truth image with the generated one over the views available in the dataset <d-cite key="numbynum"></d-cite>.
  

<!-- Citations are then used in the article body with the `<d-cite>` tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.

The citation is presented inline like this: <d-cite key="numbynum"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.
 -->

